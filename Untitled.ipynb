{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78321f0c-c19b-48a3-b075-19fd854173c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Helper function to load and preprocess the data\n",
    "def load_data(file_path):\n",
    "    data = np.loadtxt(file_path)\n",
    "    X = data[:, 0:10]  # First 10 columns are inputs\n",
    "    y = data[:, 10:17]  # Next 7 columns are outputs\n",
    "    return X, y\n",
    "\n",
    "# Load training and testing data\n",
    "X_train, y_train = load_data(\"SPC-Training.dat\")\n",
    "X_test, y_test = load_data(\"SPC-Testing.dat\")\n",
    "\n",
    "# Convert one-hot encoded outputs to class labels for certain algorithms\n",
    "y_train_labels = np.argmax(y_train, axis=1)\n",
    "y_test_labels = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Normalize the input data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Print dataset information\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "print(f\"Class distribution in training set:\")\n",
    "for i in range(7):\n",
    "    count = np.sum(y_train_labels == i)\n",
    "    print(f\"  Class {i}: {count} samples ({count/len(y_train_labels)*100:.1f}%)\")\n",
    "\n",
    "# Function to plot training history\n",
    "def plot_history(history):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='lower right')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Function to evaluate and print results\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    print(f\"\\n--- {model_name} Results ---\")\n",
    "    print(f\"Accuracy: {accuracy_score(y_true, y_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, \n",
    "                               target_names=['In Control', 'Rule 1', 'Rule 2', 'Rule 3', 'Rule 4', 'Rule 5', 'Rule 6']))\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(7)\n",
    "    plt.xticks(tick_marks, ['In Control', 'Rule 1', 'Rule 2', 'Rule 3', 'Rule 4', 'Rule 5', 'Rule 6'], rotation=45)\n",
    "    plt.yticks(tick_marks, ['In Control', 'Rule 1', 'Rule 2', 'Rule 3', 'Rule 4', 'Rule 5', 'Rule 6'])\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm.max() / 2\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "# 1. MLP Implementation\n",
    "def train_mlp():\n",
    "    print(\"\\n=== Training MLP Network ===\")\n",
    "    \n",
    "    # Calculate class weights to handle imbalance\n",
    "    class_counts = np.sum(y_train, axis=0)\n",
    "    class_weights = {i: len(y_train) / (len(class_counts) * count) for i, count in enumerate(class_counts)}\n",
    "    \n",
    "    # Create and compile the model\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Dense(32, activation='relu', input_shape=(10,)),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Hidden layers\n",
    "        Dense(64, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        Dense(32, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.2),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    # Set up callbacks\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_scaled, y_train,\n",
    "        epochs=150,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        class_weight=class_weights\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_history(history)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test_scaled)\n",
    "    y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mlp_acc = evaluate_model(y_test_labels, y_pred, \"MLP\")\n",
    "    \n",
    "    return model, mlp_acc\n",
    "\n",
    "# 2. RBF Implementation using scikit-learn's MLPClassifier with custom centers\n",
    "def train_rbf():\n",
    "    print(\"\\n=== Training RBF Network ===\")\n",
    "    \n",
    "    # Set up parameter grid for grid search\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (150,)],\n",
    "        'activation': ['tanh'],  # Using tanh to mimic RBF behavior\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate_init': [0.001, 0.01]\n",
    "    }\n",
    "    \n",
    "    # Create the MLPClassifier\n",
    "    mlp = MLPClassifier(max_iter=500, early_stopping=True, verbose=1)\n",
    "    \n",
    "    # Create grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        mlp, param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Fit the grid search\n",
    "    grid_search.fit(X_train_scaled, y_train_labels)\n",
    "    \n",
    "    # Get the best model\n",
    "    best_rbf = grid_search.best_estimator_\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_rbf.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    rbf_acc = evaluate_model(y_test_labels, y_pred, \"RBF\")\n",
    "    \n",
    "    return best_rbf, rbf_acc\n",
    "\n",
    "# 3. Custom RBF Network Implementation\n",
    "def custom_rbf():\n",
    "    from scipy.spatial.distance import cdist\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    print(\"\\n=== Training Custom RBF Network ===\")\n",
    "    \n",
    "    # Number of RBF centers for each class\n",
    "    n_centers_per_class = 15\n",
    "    total_centers = n_centers_per_class * 7  # 7 classes\n",
    "    \n",
    "    # Initialize centers and widths arrays\n",
    "    centers = np.zeros((total_centers, X_train_scaled.shape[1]))\n",
    "    classes = np.zeros(total_centers)\n",
    "    \n",
    "    # For each class, find centers using K-means\n",
    "    current_idx = 0\n",
    "    for class_idx in range(7):\n",
    "        # Get samples for this class\n",
    "        class_samples = X_train_scaled[y_train_labels == class_idx]\n",
    "        \n",
    "        # Skip if no samples (shouldn't happen with this dataset)\n",
    "        if len(class_samples) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Use K-means to find centers\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=min(n_centers_per_class, len(class_samples)),\n",
    "            random_state=42,\n",
    "            n_init=10\n",
    "        ).fit(class_samples)\n",
    "        \n",
    "        # Store centers and class labels\n",
    "        n_actual_centers = len(kmeans.cluster_centers_)\n",
    "        centers[current_idx:current_idx+n_actual_centers] = kmeans.cluster_centers_\n",
    "        classes[current_idx:current_idx+n_actual_centers] = class_idx\n",
    "        current_idx += n_actual_centers\n",
    "    \n",
    "    # Remove any unused centers\n",
    "    centers = centers[:current_idx]\n",
    "    classes = classes[:current_idx]\n",
    "    \n",
    "    # Compute average distance between centers to determine width\n",
    "    dists = cdist(centers, centers)\n",
    "    avg_dist = np.mean(dists[dists > 0])\n",
    "    width = avg_dist / np.sqrt(2 * centers.shape[0])\n",
    "    \n",
    "    # Create RBF feature transformer function\n",
    "    def rbf_transform(X):\n",
    "        dists = cdist(X, centers)\n",
    "        return np.exp(-(dists ** 2) / (2 * width ** 2))\n",
    "    \n",
    "    # Transform training and testing data\n",
    "    X_train_rbf = rbf_transform(X_train_scaled)\n",
    "    X_test_rbf = rbf_transform(X_test_scaled)\n",
    "    \n",
    "    # Train a linear model on top of RBF features\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    clf = LogisticRegression(max_iter=1000, multi_class='multinomial')\n",
    "    clf.fit(X_train_rbf, y_train_labels)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test_rbf)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    custom_rbf_acc = evaluate_model(y_test_labels, y_pred, \"Custom RBF\")\n",
    "    \n",
    "    return clf, custom_rbf_acc, centers, width\n",
    "\n",
    "# 4. SVM Implementation\n",
    "def train_svm():\n",
    "    print(\"\\n=== Training SVM ===\")\n",
    "    \n",
    "    # Create pipeline with scaler and SVM\n",
    "    svm_pipeline = Pipeline([\n",
    "        ('svm', OneVsRestClassifier(SVC(probability=True)))\n",
    "    ])\n",
    "    \n",
    "    # Define parameter grid\n",
    "    param_grid = {\n",
    "        'svm__estimator__C': [0.1, 1, 10, 100],\n",
    "        'svm__estimator__gamma': ['scale', 'auto', 0.1, 0.01],\n",
    "        'svm__estimator__kernel': ['rbf', 'poly']\n",
    "    }\n",
    "    \n",
    "    # Perform grid search\n",
    "    grid_search = GridSearchCV(\n",
    "        svm_pipeline, param_grid, cv=3, scoring='accuracy', verbose=1, n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"Starting Grid Search for SVM...\")\n",
    "    grid_search.fit(X_train_scaled, y_train_labels)\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Get best model\n",
    "    best_svm = grid_search.best_estimator_\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_svm.predict(X_test_scaled)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    svm_acc = evaluate_model(y_test_labels, y_pred, \"SVM\")\n",
    "    \n",
    "    return best_svm, svm_acc\n",
    "\n",
    "# 5. Compare all models\n",
    "def compare_models(accuracies):\n",
    "    models = list(accuracies.keys())\n",
    "    accs = list(accuracies.values())\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(models, accs, color=['blue', 'green', 'red', 'purple'])\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Model Comparison')\n",
    "    plt.ylim(0, 1.0)\n",
    "    \n",
    "    # Add accuracy values on top of bars\n",
    "    for i, v in enumerate(accs):\n",
    "        plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run all models and compare results\n",
    "def run_all():\n",
    "    accuracies = {}\n",
    "    \n",
    "    # Train and evaluate MLP\n",
    "    _, mlp_acc = train_mlp()\n",
    "    accuracies['MLP'] = mlp_acc\n",
    "    \n",
    "    # Train and evaluate RBF\n",
    "    _, rbf_acc = train_rbf()\n",
    "    accuracies['RBF'] = rbf_acc\n",
    "    \n",
    "    # Train and evaluate Custom RBF\n",
    "    _, custom_rbf_acc, _, _ = custom_rbf()\n",
    "    accuracies['Custom RBF'] = custom_rbf_acc\n",
    "    \n",
    "    # Train and evaluate SVM\n",
    "    _, svm_acc = train_svm()\n",
    "    accuracies['SVM'] = svm_acc\n",
    "    \n",
    "    # Compare all models\n",
    "    compare_models(accuracies)\n",
    "    \n",
    "    return accuracies\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
